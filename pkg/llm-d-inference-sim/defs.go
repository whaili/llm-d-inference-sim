/*
Copyright 2025 The llm-d-inference-sim Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Definitions of all structures used by vLLM simulator
// Contains the main simulator class and all definitions related to request/response for all supported APIs
package llmdinferencesim

import (
	"encoding/json"
	"errors"
	"fmt"
	"strings"
	"sync"

	"github.com/go-logr/logr"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/valyala/fasthttp"
)

const (
	modeRandom                = "random"
	modeEcho                  = "echo"
	chatComplIDPrefix         = "chatcmpl-"
	stopFinishReason          = "stop"
	lengthFinishReason        = "length"
	toolsFinishReason         = "tool_calls"
	roleAssistant             = "assistant"
	roleUser                  = "user"
	textCompletionObject      = "text_completion"
	chatCompletionObject      = "chat.completion"
	chatCompletionChunkObject = "chat.completion.chunk"
	toolChoiceNone            = "none"
	toolChoiceAuto            = "auto"
	toolChoiceRequired        = "required"
)

// VllmSimulator simulates vLLM server supporting OpenAI API
type VllmSimulator struct {
	// logger is used for information and errors logging
	logger logr.Logger
	// timeToFirstToken time before the first token will be returned, in milliseconds
	timeToFirstToken int
	// interTokenLatency time between generated tokens, in milliseconds
	interTokenLatency int
	// port defines on which port the simulator runs
	port int
	// mode defines the simulator response generation mode, valid values: echo, random
	mode string
	// model defines the current base model name
	model string
	// loraAdaptors contains list of LoRA available adaptors
	loraAdaptors sync.Map
	// maxLoras defines maximum number of loaded loras
	maxLoras int
	// maxCPULoras defines maximum number of loras to store in CPU memory
	maxCPULoras int
	// runningLoras is a collection of running loras, key of lora's name, value is number of requests using this lora
	runningLoras sync.Map
	// waitingLoras will represent collection of loras defined in requests in the queue - Not implemented yet
	waitingLoras sync.Map
	// maxRunningReqs defines the maximum number of inference requests that could be processed at the same time
	maxRunningReqs int64
	// nRunningReqs is the number of inference requests that are currently being processed
	nRunningReqs int64
	// nWaitingReqs is the number of inference requests that are waiting to be processed
	nWaitingReqs int64
	// loraInfo is prometheus gauge
	loraInfo *prometheus.GaugeVec
	// runningRequests is prometheus gauge
	runningRequests *prometheus.GaugeVec
	// waitingRequests is prometheus gauge for number of queued requests
	waitingRequests *prometheus.GaugeVec
	// kvCacheUsagePercentage is prometheus gauge
	kvCacheUsagePercentage *prometheus.GaugeVec
	// channel for requeasts to be passed to workers
	reqChan chan *completionReqCtx
	// schema validator for tools parameters
	toolsValidator *validator
}

// baseResponseChoice contains base completion response's choice related information
type baseResponseChoice struct {
	// Index defines completion response choise Index
	Index int `json:"index"`
	// FinishReason defines finish reason for response or for chunks, for not last chinks is defined as null
	FinishReason *string `json:"finish_reason"`
}

// usage contains token usage statistics
type usage struct {
	// PromptTokens is the number of tokens in the prompt
	PromptTokens int `json:"prompt_tokens"`
	// CompletionTokens is the number of tokens generated by the model as the response
	CompletionTokens int `json:"completion_tokens"`
	// TotalTokens is the total number of tokens processed for the request (the sum of the two values above)
	TotalTokens int `json:"total_tokens"`
}

// baseCompletionResponse contains base completion response related information
type baseCompletionResponse struct {
	// ID defines the response ID
	ID string `json:"id"`
	// Created defines the response creation timestamp
	Created int64 `json:"created"`
	// Model defines the Model name for current request
	Model string `json:"model"`
	// Usage contains the token usage statistics for the request
	Usage *usage `json:"usage"`
	// Object is the Object type, "text_completion", "chat.completion", or "chat.completion.chunk"
	Object string `json:"object"`
}

// completionResponse interface representing both completion response types (text and chat)
type completionResponse interface {
}

// StreamOptions defines streaming options for streaming requests
type streamOptions struct {
	// IncludeUsage is a boolean value, defines whether response contain usage statistics
	IncludeUsage bool `json:"include_usage"`
}

// baseCompletionRequest contains base completion request related information
type baseCompletionRequest struct {
	// Stream is a boolean value, defines whether response should be sent as a Stream
	Stream bool `json:"stream"`
	// StreamOptions defines streaming options in case Stream is set to true
	StreamOptions streamOptions `json:"stream_options"`
	// Model defines Model name to use for "inference", could be base Model name or one of available LoRA adapters
	Model string `json:"model"`
}

func (b *baseCompletionRequest) isStream() bool {
	return b.Stream
}

func (b *baseCompletionRequest) getModel() string {
	return b.Model
}

func (b *baseCompletionRequest) includeUsage() bool {
	return !b.Stream || b.StreamOptions.IncludeUsage
}

// completionRequest interface representing both completion request types (text and chat)
type completionRequest interface {
	// createResponseText creates and returns response payload based on this request,
	// and the finish reason
	createResponseText(mode string) (string, string, error)
	// createToolCalls creates and returns response payload based on this request
	// (tool calls or nothing in case we randomly choose not to generate calls), and the finish reason
	createToolCalls() ([]toolCall, string, error)
	// isStream returns boolean that defines is response should be streamed
	isStream() bool
	// getModel returns model name as defined in the request
	getModel() string
	// includeUsage returns true if usage statistics should be include in the response
	includeUsage() bool
	// getNumberOfPromptTokens returns the number of tokens in the prompt
	getNumberOfPromptTokens() int
	// getTools() returns tools to use (in chat completion)
	getTools() []tool
	// getToolChoice() returns tool choice (in chat completion)
	getToolChoice() string
}

type completionReqCtx struct {
	completionReq    completionRequest
	httpReqCtx       *fasthttp.RequestCtx
	isChatCompletion bool
	wg               *sync.WaitGroup
}

// v1/chat/completion
// message defines vLLM chat completion message
type message struct {
	// Role is the message Role, optional values are 'user', 'assistant', ...
	Role string `json:"role,omitempty"`
	// Content defines text of this message
	Content content `json:"content,omitempty"`
	// ToolCalls are the tool calls created by the model
	ToolCalls []toolCall `json:"tool_calls,omitempty"`
}

type content struct {
	Raw        string
	Structured []contentBlock
}

type contentBlock struct {
	Type     string     `json:"type"`
	Text     string     `json:"text,omitempty"`
	ImageURL ImageBlock `json:"image_url,omitempty"`
}

type ImageBlock struct {
	Url string `json:"url,omitempty"`
}

// UnmarshalJSON allow use both format
func (mc *content) UnmarshalJSON(data []byte) error {
	// Raw format
	var str string
	if err := json.Unmarshal(data, &str); err == nil {
		mc.Raw = str
		return nil
	}

	// Block format
	var blocks []contentBlock
	if err := json.Unmarshal(data, &blocks); err == nil {
		mc.Structured = blocks
		return nil
	}

	return errors.New("content format not supported")
}

func (mc content) MarshalJSON() ([]byte, error) {
	if mc.Raw != "" {
		return json.Marshal(mc.Raw)
	}
	if mc.Structured != nil {
		return json.Marshal(mc.Structured)
	}
	return json.Marshal("")
}

func (mc content) PlainText() string {
	if mc.Raw != "" {
		return mc.Raw
	}
	var sb strings.Builder
	for _, block := range mc.Structured {
		if block.Type == "text" {
			sb.WriteString(block.Text)
			sb.WriteString(" ")
		}
	}
	return sb.String()
}

// function defines a tool
type function struct {
	// Name is the function's name
	Name string `json:"name"`
	// Parameters are the parameters the function accepts
	Parameters map[string]any `json:"parameters,omitempty"`
	// Description is the function's description
	Description string `json:"description"`
}

// tool defines a tool to use in chat completion
type tool struct {
	// Function describes the tool
	Function function `json:"function"`
	// Type defines the type of the tool, currently only functions are
	// supported by vLLM
	Type string `json:"type"`
}

// chatCompletionRequest defines structure of /chat/completion request
type chatCompletionRequest struct {
	baseCompletionRequest
	// Messages list of request's Messages
	Messages []message `json:"messages"`

	// The maximum number of tokens that can be generated in the chat
	// completion. This value can be used to control costs for text
	// generated via API.
	// This value is now deprecated in favor of max_completion_tokens
	// and is not compatible with o1 series models.
	MaxTokens *int64 `json:"max_tokens"`

	// An upper bound for the number of tokens that can be
	// generated for a completion, including visible output
	// tokens and reasoning tokens.
	MaxCompletionTokens *int64 `json:"max_completion_tokens"`

	// Tools is a list of tools the model may call.
	Tools []tool `json:"tools,omitempty"`

	// ToolChoice controls which (if any) tool is called by the model,
	// possible values: none, auto, required.
	// Sending an object with a specific tool, is currently not supported.
	ToolChoice string `json:"tool_choice,omitempty"`
}

func (c *chatCompletionRequest) getNumberOfPromptTokens() int {
	var messages string
	for _, message := range c.Messages {
		messages += message.Content.PlainText() + " "
	}
	return len(strings.Fields(messages))
}

func (c *chatCompletionRequest) getTools() []tool {
	return c.Tools
}

func (c *chatCompletionRequest) getToolChoice() string {
	return c.ToolChoice
}

// functionCall defines a tool call generated by the model including its arguments
type functionCall struct {
	// Arguments are the arguments of the function call
	Arguments string `json:"arguments,omitempty"`
	// Name is the function's name, can be null in streaming in not the first chunk
	Name *string `json:"name"`
}

// toolCall defines a tool call generated by the model
type toolCall struct {
	// Function is a tool call generated by the model
	Function functionCall `json:"function"`
	// ID is the ID of the tool call
	ID string `json:"id"`
	// Type is the type of the tool, only functions are supported
	Type string `json:"type"`
	// Index is the index of the tool in the sequence of tools generated by the model
	Index int `json:"index"`
}

// chatCompletionResponse defines structure of /chat/completion response
type chatCompletionResponse struct {
	baseCompletionResponse
	// Choices list of Choices of the response, according of OpenAI API
	Choices []chatRespChoice `json:"choices"`
}

// chatRespChoice represents a single chat completion response choise
type chatRespChoice struct {
	baseResponseChoice
	// Message contains choice's Message
	Message message `json:"message"`
}

// v1/completion
// textCompletionRequest defines structure of /completion request
type textCompletionRequest struct {
	baseCompletionRequest
	// Prompt defines request's content
	Prompt string `json:"prompt"`

	// The maximum number of [tokens](/tokenizer) that can be generated in the
	// completion.
	//
	// The token count of your prompt plus `max_tokens` cannot exceed the model's
	// context length.
	MaxTokens *int64 `json:"max_tokens"`
}

func (t *textCompletionRequest) getNumberOfPromptTokens() int {
	return len(strings.Fields(t.Prompt))
}

func (c *textCompletionRequest) getTools() []tool {
	return nil
}

func (c *textCompletionRequest) getToolChoice() string {
	return ""
}

// textCompletionResponse defines structure of /completion response
type textCompletionResponse struct {
	baseCompletionResponse
	// Choices list of Choices of the response, according of OpenAI API
	Choices []textRespChoice `json:"choices"`
}

// textRespChoice represents a single text completion response choise
type textRespChoice struct {
	baseResponseChoice
	// Text defines request's content
	Text string `json:"text"`
}

// completionRespChunk is an interface that defines a single response chunk
type completionRespChunk interface{}

// chatCompletionRespChunk is a single chat completion response chunk
type chatCompletionRespChunk struct {
	baseCompletionResponse
	// Choices list of Choices of the response, according of OpenAI API
	Choices []chatRespChunkChoice `json:"choices"`
}

// chatRespChunkChoice represents a single chat completion response choise in case of streaming
type chatRespChunkChoice struct {
	baseResponseChoice
	// Delta is a content of the chunk
	Delta message `json:"delta"`
}

// returns the max. tokens or error if incorrect
func getMaxTokens(maxCompletionTokens *int64, maxTokens *int64) (*int64, error) {
	var typeToken string
	var tokens *int64
	// if both arguments are passed,
	// use maxCompletionTokens
	// as in the real vllm
	if maxCompletionTokens != nil {
		tokens = maxCompletionTokens
		typeToken = "max_completion_tokens"
	} else if maxTokens != nil {
		tokens = maxTokens
		typeToken = "max_tokens"
	}
	if tokens != nil && *tokens < 1 {
		return nil, fmt.Errorf("%s must be at least 1, got %d", typeToken, *tokens)
	}
	return tokens, nil
}

// createResponseText creates response text for the given chat completion request and mode
func (req chatCompletionRequest) createResponseText(mode string) (string, string, error) {
	maxTokens, err := getMaxTokens(req.MaxCompletionTokens, req.MaxTokens)
	if err != nil {
		return "", "", err
	}

	var text, finishReason string
	if mode == modeEcho {
		text, finishReason = getResponseText(maxTokens, req.getLastUserMsg())
	} else {
		text, finishReason = getRandomResponseText(maxTokens)
	}

	return text, finishReason, nil
}

// createResponseText creates response text for the given text completion request and mode
func (req textCompletionRequest) createResponseText(mode string) (string, string, error) {
	maxTokens, err := getMaxTokens(nil, req.MaxTokens)
	if err != nil {
		return "", "", err
	}

	var text, finishReason string
	if mode == modeEcho {
		text, finishReason = getResponseText(maxTokens, req.Prompt)
	} else {
		text, finishReason = getRandomResponseText(maxTokens)
	}

	return text, finishReason, nil
}

// createToolCalls creates and returns response payload based on this request
// (tool calls or nothing in case we randomly choose not to generate calls), and the finish reason
func (req chatCompletionRequest) createToolCalls() ([]toolCall, string, error) {
	// This function is called if tool choice is either 'required' or 'auto'.
	// In case of 'required' at least one tool call has to be created, and we randomly choose
	// the number of calls starting from one. Otherwise, we start from 0, and in case we randomly
	// choose the number of calls to be 0, response text will be generated instead of a tool call.
	numberOfCalls := randomInt(len(req.Tools), req.ToolChoice == toolChoiceRequired)
	if numberOfCalls == 0 {
		return nil, "", nil
	}

	calls := make([]toolCall, 0)
	for i := range numberOfCalls {
		// Randomly choose which tools to call. We may call the same tool more than once.
		index := randomInt(len(req.Tools)-1, false)
		args, err := generateToolArguments(req.Tools[index])
		if err != nil {
			return nil, "", err
		}
		argsJson, err := json.Marshal(args)
		if err != nil {
			return nil, "", err
		}

		call := toolCall{
			Function: functionCall{
				Arguments: string(argsJson),
				Name:      &req.Tools[index].Function.Name,
			},
			ID:    "chatcmpl-tool-" + randomNumericString(10),
			Type:  "function",
			Index: i,
		}
		calls = append(calls, call)
	}
	return calls, toolsFinishReason, nil
}

// createToolCalls shouldn't be called for text completion
func (req textCompletionRequest) createToolCalls() ([]toolCall, string, error) {
	return nil, "", errors.New("tool calls are not supported in text completion")
}

// getLastUserMsg returns last message from this request's messages with user role,
// if does not exist - returns an empty string
func (req *chatCompletionRequest) getLastUserMsg() string {
	for i := len(req.Messages) - 1; i >= 0; i-- {
		if req.Messages[i].Role == roleUser {
			return req.Messages[i].Content.PlainText()
		}
	}

	return ""
}

// completionError defines structure of error returned by completion request
type completionError struct {
	// Object is a type of this Object, "error"
	Object string `json:"object"`
	// Message is an error Message
	Message string `json:"message"`
	// Type is a type of the error
	Type string `json:"type"`
	// Params is the error's parameters
	Param *string `json:"param"`
	// Code is http status Code
	Code int `json:"code"`
}

type loadLoraRequest struct {
	LoraName string `json:"lora_name"`
	LoraPath string `json:"lora_path"`
}

type unloadLoraRequest struct {
	LoraName string `json:"lora_name"`
}

func (s *VllmSimulator) getLoras() []string {

	loras := make([]string, 0)

	s.loraAdaptors.Range(func(key, _ any) bool {
		if lora, ok := key.(string); ok {
			loras = append(loras, lora)
		}
		return true
	})

	return loras
}

func (s *VllmSimulator) addLora(lora string) {
	s.loraAdaptors.Store(lora, "")
}

func (s *VllmSimulator) removeLora(lora string) {
	s.loraAdaptors.Delete(lora)
}
